#!/usr/bin/env python3

import sys, os, json, re, random, time
from pathlib import Path
from typing import List, Dict, Any
from collections import defaultdict

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))

from baseutils.ioutils import get_all_solutions, Solution
from LLMRequest.askLLM import ask_LLM
from sklearn.model_selection import train_test_split
import numpy as np
from tqdm import tqdm
from Zeroshot.GPTJudge.analyze_cached_results import pct
from transformers import AutoTokenizer


def _build_instruction(lang: str, base_prompt: str | None, include_description: bool) -> str:
    if base_prompt:
        return base_prompt
    if include_description:
        return (
            f"You will be provided with a task description and a {lang} source code. Please determine if the source code is generated by Artificial intelligence (AI) models (i.e. language models) or human. "
            f"If the code is generated by AI, output 1. If the code is generated by human, output 0. Return the label only in the output."
        )
    else:
        return (
            f"You will be provided with a {lang} source code. Please determine if the source code is generated by Artificial intelligence (AI) models (i.e. language models) or human. "
            f"If the code is generated by AI, output 1. If the code is generated by human, output 0. Return the label only in the output."
        )


def build_judge_prompt(solution: Solution, use_description: bool, base_prompt: str | None, concat_mode: str = "simple") -> list[dict]:
    lang = solution.language
    code = solution.code
    if use_description and solution.description:
        if concat_mode == "simple":
            content = f"Task Description:\n{solution.description}\n\nSource Code ({lang}):\n``" + "`\n" + code + "\n``" + "`\n"
        else:
            content = f"Task Description:\n{solution.description}\n\nSource Code ({lang}):\n``" + "`\n" + code + "\n``" + "`\n"
    else:
        content = f"Source Code ({lang}):\n``" + "`\n" + code + "\n``" + "`\n"

    instruction = _build_instruction(lang, base_prompt, include_description=bool(use_description and solution.description))

    messages = [
        {"role": "system", "content": "You are a precise code-origin detector."},
        {"role": "user", "content": instruction + "\n\n" + content}
    ]
    return messages


def parse_label_only(output: str) -> int | None:
    if output is None:
        return None
    text = output.strip()
    m = re.search(r"[01]", text)
    if m:
        return int(m.group(0))
    return None


def get_lang_entry(cache_data: Dict, language: str, model: str) -> Dict | None:
    for entry in cache_data.get("solutions", []):
        if entry.get("language") == language and entry.get("original_model") == model:
            return entry
    return None


def save_judgement(cache_root: Path, solution: Solution, judge_model: str, pred_label: int, raw_output: str, use_description: bool, concat_mode: str):
    label_str = "ai" if solution.label == 1 else "human"
    task_dir = cache_root / solution.task_name
    task_dir.mkdir(parents=True, exist_ok=True)
    json_path = task_dir / f"{solution.task_name}_{label_str}.json"

    if json_path.exists():
        try:
            cache = json.loads(json_path.read_text())
        except json.JSONDecodeError:
            cache = {}
    else:
        cache = {"task_name": solution.task_name, "task_description": solution.description, "solutions": []}

    lang_entry = get_lang_entry(cache, solution.language, solution.model)
    if not lang_entry:
        lang_entry = {
            "language": solution.language,
            "original_model": solution.model,
            "judgements": []
        }
        cache.setdefault("solutions", []).append(lang_entry)

    judgement = {
        "judge_model": judge_model,
        "use_description": use_description,
        "concat_mode": concat_mode,
        "prediction_label": pred_label,
        "yes_no": ("yes" if pred_label == 1 else "no"),
        "raw_output": raw_output,
    }
    lang_entry["judgements"].append(judgement)

    json_path.write_text(json.dumps(cache, ensure_ascii=False, indent=2))


def load_cached_judgement(cache_root: Path, solution: Solution, judge_model: str, use_description: bool, concat_mode: str) -> tuple[int, str] | None:
    label_str = "ai" if solution.label == 1 else "human"
    json_path = cache_root / solution.task_name / f"{solution.task_name}_{label_str}.json"
    if not json_path.exists():
        return None
    try:
        cache = json.loads(json_path.read_text())
    except json.JSONDecodeError:
        return None
    lang_entry = get_lang_entry(cache, solution.language, solution.model)
    if not lang_entry:
        return None
    if lang_entry.get("original_model") != solution.model:
        return None
    for j in lang_entry.get("judgements", []):
        if j.get("judge_model") == judge_model and j.get("use_description") == use_description and j.get("concat_mode") == concat_mode:
            return j.get("prediction_label"), j.get("raw_output")
    return None


def zero_shot_evaluate_within_across(
    solutions: List[Solution],
    preds: np.ndarray,
    judge_name: str,
    *,
    random_state: int = 42,
):
    languages = sorted(list(set(s.language for s in solutions)))
    rs = np.random.RandomState(random_state)

    print("\n" + "="*22 + f" Zero-Shot Evaluate ({judge_name}) " + "="*22)
    for lang in languages:
        lang_idx = [i for i, s in enumerate(solutions) if s.language == lang]
        if len(lang_idx) < 2:
            continue

        print(f"\n--- Language: {lang} ---")
        judge_target_model = judge_name

        within_idx = [i for i in lang_idx if solutions[i].model == judge_target_model or solutions[i].label == 0]
        if len(within_idx) >= 2:
            y_true = np.array([solutions[i].label for i in within_idx])
            y_pred = preds[within_idx]
            from sklearn.metrics import classification_report
            print("  > Within (all samples)")
            print(classification_report(y_true, y_pred, digits=4, zero_division=0))
        else:
            print("  > Within: insufficient samples.")

        ai_other = [i for i in lang_idx if (solutions[i].model != "Human" and solutions[i].model != judge_target_model)]
        human = [i for i in lang_idx if solutions[i].label == 0]
        min_len = min(len(ai_other), len(human))
        if min_len >= 1:
            ai_other_bal = rs.permutation(ai_other)[:min_len]
            human_bal = rs.permutation(human)[:min_len]
            across_idx = np.concatenate([ai_other_bal, human_bal])
            y_true = np.array([solutions[i].label for i in across_idx])
            y_pred = preds[across_idx]
            from sklearn.metrics import classification_report
            print("  > Across (balanced AI-other vs Human)")
            print(classification_report(y_true, y_pred, digits=4, zero_division=0))
        else:
            print("  > Across: insufficient samples.")


def _sanitize_for_path(x: str | None) -> str:
    if not x:
        return "none"
    return str(x).replace("/", "-").replace("\\", "-").replace(" ", "_")


def _compose_cache_subdir(config: Dict[str, Any]) -> str:
    use_description = bool(config.get('use_description', False))
    concat_mode = config.get('concat_mode', 'simple')
    prompt_template_name = _sanitize_for_path(config.get('prompt_template_name', None))
    desc_processor_name = _sanitize_for_path(config.get('desc_processor_name', None))

    parts = [
        f"desc_{'on' if use_description else 'off'}",
        f"cm_{_sanitize_for_path(concat_mode)}",
        f"tmpl_{prompt_template_name}",
        f"proc_{desc_processor_name}",
    ]
    return "__".join(parts)


def _messages_to_text(messages: list[dict]) -> str:
    return "\n\n".join([f"{m.get('role','user')}:\n{m.get('content','')}" for m in messages])


def _build_tokenizer(tokenizer_path: str):
    try:
        tok = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=True, trust_remote_code=True)
        return tok
    except Exception:
        return None


def _print_distribution(solutions: List[Solution]):
    total = len(solutions)
    print("\n" + "-" * 16 + " Distribution (solutions-level) " + "-" * 16)
    print(f"Total solutions: {total}")

    from collections import defaultdict
    dataset_counts = defaultdict(int)
    language_counts = defaultdict(int)
    nested = defaultdict(lambda: defaultdict(lambda: {"count": 0, "orig_models": defaultdict(int)}))

    for s in solutions:
        ds = (s.task_name.split('_', 1)[0] if '_' in s.task_name else "unknown").lower()
        lang = s.language or "unknown"
        model = s.model or "unknown"
        dataset_counts[ds] += 1
        language_counts[lang] += 1
        nested[ds][lang]["count"] += 1
        nested[ds][lang]["orig_models"][model] += 1

    print("\n- Dataset proportions:")
    for ds, cnt in sorted(dataset_counts.items(), key=lambda x: (-x[1], x[0])):
        print(f"  {ds:8s}  {cnt:6d}  ({pct(cnt, total)})")

    lang_total = sum(language_counts.values())
    print("\n- Language proportions:")
    for lang, cnt in sorted(language_counts.items(), key=lambda x: (-x[1], x[0])):
        print(f"  {lang:8s}  {cnt:6d}  ({pct(cnt, lang_total)})")

    print("\n- Per (dataset Ã— language): original model distribution:")
    for ds in sorted(nested.keys()):
        for lang in sorted(nested[ds].keys()):
            group = nested[ds][lang]
            group_count = group["count"]
            print(f"\n  [{ds} | {lang}]  solutions={group_count}")
            orig_models = group["orig_models"]
            if orig_models:
                for model_name, cnt in sorted(orig_models.items(), key=lambda x: (-x[1], x[0])):
                    print(f"    - {model_name:24s} count={cnt:5d} ({pct(cnt, group_count)})")
            else:
                print("    (no original model info)")

def run(**config):
    solutions = get_all_solutions(config['root_path'])

    cache_subdir = _compose_cache_subdir(config)
    cache_dir = Path(config.get('cache_dir', './zero_shot_judgements')) / cache_subdir
    cache_dir.mkdir(exist_ok=True, parents=True)

    judge_model = config.get('judge_model') or (config.get('judge_models', ["gpt-4o"])[0])
    use_description = config.get('use_description', False)
    base_prompt = config.get('prompt', None)
    concat_mode = config.get('concat_mode', 'simple')

    print("[ZeroShot] Sampling disabled: using all solutions.")

    cache_subdir = _compose_cache_subdir(config)
    cache_dir = Path(config.get('cache_dir', './zero_shot_judgements')) / cache_subdir
    cache_dir.mkdir(exist_ok=True, parents=True)

    temperature = float(config.get('temperature', 0.0))
    max_retry = int(config.get('max_retry', 3))

    per_solution_seconds: List[float] = []
    per_ds_seconds: Dict[str, list] = defaultdict(list)
    print(f"\n===== Judge (single): {judge_model} =====")
    for s in tqdm(solutions, desc=f"LLM requests ({judge_model})", total=len(solutions)):
        try:
            ds = _dataset_of_task(s.task_name)
            t0 = time.perf_counter()
            messages = build_judge_prompt(s, use_description, base_prompt, concat_mode)
            reply = ask_LLM(judge_model, messages, temperature, max_retry)
            label = parse_label_only(reply)
            if label is None:
                label = 1
            save_judgement(cache_dir, s, judge_model, int(label), reply, use_description, concat_mode)
            dt = time.perf_counter() - t0
            per_solution_seconds.append(dt)
            per_ds_seconds[ds].append(dt)
            print(f"  - {ds:7s} | {s.task_name:20s} | {s.language:6s} | orig={s.model:12s} -> pred={int(label)} | {dt:.3f}s")
        except Exception as e:
            print(f"  [WARN] Judge failed for {s.task_name}/{s.language}/{s.model}: {e}")
            dt = time.perf_counter() - t0
            per_solution_seconds.append(dt)
            per_ds_seconds[_dataset_of_task(s.task_name)].append(dt)

    if per_solution_seconds:
        overall_avg = sum(per_solution_seconds) / len(per_solution_seconds)
        print("\n" + "="*20 + " Per-Solution Detection Time " + "="*20)
        for ds, lst in per_ds_seconds.items():
            if lst:
                print(f"  {ds:8s} avg: {sum(lst)/len(lst):.3f}s  (N={len(lst)})")
        print(f"  overall  avg: {overall_avg:.3f}s  (N={len(per_solution_seconds)})")
    else:
        print("[ZeroShot] No solutions processed.")


def run_all():
    base = {
        'root_path': "../../splitDescription/split_benchmark",
        'cache_dir': "./zero_shot_judgement",
        'judge_model': "gpt-4o",
        'use_description': False,
        'prompt': None,
        'temperature': 0.0,
        'max_retry': 3,
        'concat_mode': 'simple',
        'random_state': 42,
    }
    run(**base)


if __name__ == "__main__":
    run_all()

